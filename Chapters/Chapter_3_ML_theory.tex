% Things that I have to cover:
% 1. Intro: Basics + History
% 2. Supervised Learning
% 3. Overfitting + Underfitting
% 4. Perceptron + Activation Functions
% 5. Loss Functions
% 6. Backpropagation
% 7. Optimization
% 8. Linear Regression
% 9. Neural Networks
% 10. Normalization
% 11. Gaussian Processes

In this chapter, we will discuss the machine learning techniques employed in this work. It does not aim to be a comprehensive guide to the subject, but rather a summary of the most important concepts and techniques so that the reader can understand the main ideas and the choices made in this work. One can refer to \cite{pml1Book} for a more detailed and comprehensive guide to the subject.

\section{Introduction}
In the recent years, machine learning, and particularly deep learning, has become a powerful tool in every scientific field. Some common examples include Large Language Models (LLMs) \cite{radford2019language,touvron2023llamaopenefficientfoundation}, synthetic image generation \cite{ho2020denoisingdiffusionprobabilisticmodels}, stellar astronomy \cite{ostdiek_b_cataloging_2020}, geoinformatics \cite{robin2022learningforecastvegetationgreenness}.

Let's begin by defining machine learning in a general way:

\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black]
    An algorithm which is used to extract patterns and relationships from data, to make predictions, and to solve problems that are too complex to be solved by traditional methods.
\end{tcolorbox}

Since the development of advanced engineering tools and numerical programming, we have access to huge amount of data, and machine learning helps us to "learn" an intrinsic structure of it. 

This "learning" can be done in different ways, and we broadly classify them into three categories:
\begin{itemize}
    \item Supervised Learning
    \item Unsupervised Learning
    \item Reinforcement Learning
\end{itemize}

Since, our work focuses on the supervised learning, we will only discuss it in this chapter. However, later in this chapter, we will provide a brief overview of the other two types of learning.

\subsection{Supervised Learning}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.70]{chapter_03_ML/supervised_learn_pipeline.pdf} % [width=7cm, height=5cm]
    \caption{Supervised learning pipeline.}
    \label{fig:ml_pipeline_basic}
\end{figure}

The most common type of machine learning algorithm is the supervised learning. It is used when we have a dataset with labeled examples, and we want to learn a function $f$ that maps inputs $\textbf{x}$ to outputs $\textbf{y}$. Some examples of supervised learning problems include spam email detection, image classification, and speech recognition. The figure \ref{fig:ml_pipeline_basic} summarizes a typical supervised learning workflow, where the labeled training data is passed to a machine learning algorithm for fitting a predictive model that can make predictions on new, unlabeled data inputs.

Formally, the supervised learning problem can be written as:
\begin{equation}
    \label{eq:supervised_learning}
    \hat{\textbf{y}} = f(\textbf{x}; \theta).
\end{equation}

Here, both the input $\textbf{x}$ and output $\textbf{y}$ are vectors of a predetermined and fixed size. The model is just a mathematical equation with a fixed form. It represents a family of different relations between the input and the output. The model also contains \textit{parameters} $\theta$. The choice of parameters determines the particular relation between input and output.

When we talk about \textit{learning}, we mean that the model is able to find the \textbf{best parameters} $\theta$ that gives meaningful output for a given input. When we say meaningful, we mean that the model is able to generalize to new examples that it has not seen before. 

During the learning/training process, we learn the parameters using a set of training data of $N$ pairs of input and output $\{\textbf{x}_i, \textbf{y}_i\}$. We aim to modify these parameters such that map of each trainining input to its associated output as closely as possible. Therefore, in order to quantify the deviation between the predicted output and the target output, so that we can guide the training process, we define a \textit{loss function} $\mathcal{L}$. When we train the model, we aim to minimize this loss function:

\begin{equation}
    \theta^* = \operatorname*{argmin}_\theta \mathcal{L}(\mathcal{D}, \theta).
    \label{eq:argmin_loss}
\end{equation}

There is a systematical way called \textit{optimization algorithm} to find the best parameters $\theta^*$ that help the model reach global or local minimum of the loss function. This will be discussed later in this chapter. 

Let's move on to discuss about the term dataset which comes up often in the context of machine learning.

\section{Dataset: Training, Validation, and Test Sets}

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.70]{chapter_03_ML/train_test_val_pipeline.pdf}
    \caption{Use of training, validation, and test sets.}
    \label{fig:dataset_split}
\end{figure}

The quality of the data and the amount of useful information that it contains are key factors that determine how well a machine learning algorithm can learn. Similarly, it is extremely necessary to split the collected data into different sets so that we can evaluate the performance of the model on unseen data. Based on the purpose, most of the machine learning pipelines are designed to split the data into three sets: training set, validation set, and test set.

\begin{itemize}
    \item Training set: This is the subset of data used to fit the model. Generally speaking, we randomly pick 70\% or 80\% of the data for training.
    \item Validation set: One of the most important sets which is often overlooked. Before the training process, we fix some of the parameters of the model called \textit{hyperparameters}. The validation set is used to tune these hyperparameters and plays a key role in diagnosing issues such as \textit{overfitting} and \textit{underfitting}. We split 15\% or 10\% of the data for validation.
    \item Test set: Finally, this is set of data which my model has not seen before. It serves as the final benchmark to assess the modelâ€™s performance, robustness, and generalization capability. Like the validation set, it typically comprises 10\% to 15\% of the total data. 
\end{itemize}

The role of each set can be clearly seen in the figure \ref{fig:dataset_split}.

In the next section, we will discuss about a point mentioned in the paragraph above: \textit{underfitting} and \textit{overfitting}.

\section{Headache: Overfitting and Underfitting}
In the world of machine learning, we rely on building a network of some fundamental building blocks (usually called \textit{neurons}) and achieving the right model complexity is an ongoing struggle. To give a brief idea about these concepts, we can say that \textit{overfitting} is when the model is "too complex" and it fits the training data too well, while \textit{underfitting} is when the model is "too simple" and it does not fit the training data well enough.

Let's take an example were we sample some data points from a 4th order polynomial function with some noise. Our goal is to develop a model that learns the underlying function (not exactly, but close enough). Now, while performing the training, we can have two different scenarios, one where model complexity is too low and it returns a quadratic function, and the other where model complexity is too high and it returns the 10th order polynomial function.

Overfitting corresponds to the case where the model output 10th order polynomial function, meaning it performs well on training data but does not generalize well to unseen data (test data). If a model suffers from overfitting, we also say that the model has a high variance, which can be caused by having too many parameters, leading to a model that is too complex given the underlying data.

Underfitting, on the other hand, corresponds to the case where the model output a linear function, meaning it performs poorly on both training and test data. If a model suffers from underfitting, we also say that the model has a high bias, which can be caused by having too few parameters, leading to a model that is too simple given the underlying data.

We can clearly see in the figure \ref{fig:overfitting_underfitting} the trade-off between model complexity and generalization, with overfitting resulting in excessive sensitivity to noise, and underfitting failing to capture the true underlying trend.

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.70]{chapter_03_ML/over_under_fit.pdf}
    \caption{Illustration of model underfitting, ideal fitting, and overfitting on noisy data generated from a 4th-order polynomial function. The data points (blue scatter) are sampled from the function $0.5x^4 - x^3 - x^2 + x + 2$ with some noise $\mathcal{N}(0, 10)$. The orange curve represents an underfit model (linear regression), the green curve shows the ideal 4th-order polynomial fit, and the red curve illustrates an overfit model, where a higher-degree polynomial captures the noise.}
    \label{fig:overfitting_underfitting}
\end{figure}

I want to make one thing clear at this point. There are certain keywords which have not been clearly defined in our discussion above. However, we will follow up on them in the upcoming sections. For now, let's move on to the next section where we will discuss about the heart of the machine learning model: \textit{neural networks}.

\section{Neural Networks}

The core idea of modern machine learning or deep learning lies in the idea of building effective and problem-specific neural networks. Let's take a look at what we mean by it and what is it's origin.

\subsection{Building Blocks: Perceptrons}

The origins of neural networks (or artificial neural networks) can be traced to the early 20th century, inspired by the biological neural networks that underpin cognitive functions in living organisms. This early work lead to the development of the perceptron, which is a simple model of a neuron (artificial neuron). This forms the basis of the modern neural networks where we have multiple layers of perceptrons.

A single perceptron is a simple model that takes multiple inputs, $\textbf{x} \in \mathbb{R}^n$, and produces a single scalar output, $y$. The output is a weighted sum of the inputs, plus a bias term is passed through an \textit{activation function}, $\sigma: \mathbb{R} \rightarrow \mathbb{R}$.

\begin{equation}
    y = \sigma \left( \textbf{w}^{\top} \textbf{x} + b \right)
    \label{eq:perceptron}
\end{equation}

where, $\textbf{w} \in \mathbb{R}^n$ is the weight vector and $b \in \mathbb{R}$ is the bias term.

\subsection{Multi-layer Perceptrons}


When we have multiple perceptrons, we can stack them up to form what we call a \textit{neural network}. The output of the first layer is the input of the second layer, and so on. This way, we can build a deep neural network. The layers between the input and output layers are called \textit{hidden layers}. We start with inputs $\textbf{x} \in \mathbb{R}^n$ and we have $L$ hidden layers. The output of the $l$-th hidden layer is given by:

\begin{equation}
    \textbf{h}_l = \sigma \left( \textbf{W}_l \textbf{h}_{l-1} + \textbf{b}_l \right)
    \label{eq:hidden_layer}
\end{equation}

where, $\textbf{W}_l \in \mathbb{R}^{n \times n_{l-1}}$ is the weight matrix and $\textbf{b}_l \in \mathbb{R}^{n_l}$ is the bias vector. It is these \textit{weights} and \textit{biases} which are the \textit{parameters} tuned during the training process.

As we see in the equation \ref{eq:perceptron} and \ref{eq:hidden_layer}, we have an entity called the activation function $\sigma$. It is this function which introduces non-linearity to the model. One can make multiple choices for the activation functions which mainly depends on the problem at hand. We will highlight some of them in the next section. 

\begin{figure}[t]
    \centering
    \includegraphics[scale=0.70]{chapter_03_ML/activations_func.pdf}
    \caption{Activation functions.}
    \label{fig:activation_functions}
\end{figure}

\subsection{Activation Functions}

A stack of linear layers is equivalent to a single linear layer where we multliply together all the weight matrices. To get more expressive power we can transform each layer by passing it elementwise (pointwise) through a nonlinear function called an activation function. In figure \ref{fig:activation_functions} and table \ref{tab:activation_functions}, we have highlighted some of the most common activation functions.

Mathematically, a suitable activation function is typically required to be non-linear and differentiable to allow effective learning through gradient-based optimization methods like \textit{backpropagation}. Additional desirable properties include boundedness (to prevent uncontrolled output growth), monotonicity (to aid convergence), and computational efficiency (to ensure scalability).

Just like previous sections, there are a few keywords which have not been defined in proper manner. I just want to make sure that reader is aware that they will be discussed in the upcoming sections.

\begin{table}[t]
    \centering
    \caption{Common Activation Functions in Neural Networks}
    \renewcommand{\arraystretch}{1.5} % Increases row height
    \begin{tabular}{|>{\centering\arraybackslash}m{3.5cm}|>{\centering\arraybackslash}m{5.5cm}|>{\centering\arraybackslash}m{5.5cm}|}
    \hline
    \textbf{Activation Function} & \textbf{Expression} & \textbf{Key Properties} \\
    \hline
    Sigmoid & 
    $\displaystyle \sigma(x) = \frac{1}{1 + e^{-x}}$ & 
    Bounded, smooth, vanishing gradients \\
    \hline
    Tanh & 
    $\displaystyle \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ & 
    Zero-centered, smooth, vanishing gradients \\
    \hline
    ReLU & 
    $\displaystyle \text{ReLU}(x) = \max(0, x)$ & 
    Sparse activations, fast computation, non-saturating \\
    \hline
    Leaky ReLU & 
    $\displaystyle \text{LeakyReLU}(x) = \begin{cases}
    x, & x > 0 \\
    \alpha x, & x \leq 0
    \end{cases}$ & 
    Avoids dead neurons, small negative slope \\
    \hline
    ELU & 
    $\displaystyle \text{ELU}(x) = \begin{cases}
    x, & x > 0 \\
    \alpha(e^x - 1), & x \leq 0
    \end{cases}$ & 
    Smooth, avoids vanishing gradient for $x < 0$ \\
    \hline
    \end{tabular}
    \label{tab:activation_functions}
\end{table}

\subsection{Loss Functions}

So far its quite clear, in supervised learning, a model is trained to approximate a mapping from input data to target outputs. The loss function, also referred to as the \textbf{objective function,} plays a critical role in this process by quantifying the discrepancy between the model's predictions and the actual labels. Minimizing this loss function is central to the optimization procedure that drives the learning process.

Mathematically, given a dataset \( \mathcal{D} = \{(x_i, y_i)\}_{i=1}^N \), where \( x_i \in \mathbb{R}^d \) represents the input features and \( y_i \in \mathbb{R} \) (or \( \mathbb{R}^k \) in the case of multi-class problems) denotes the corresponding label, the model output is denoted as \( \hat{y}_i = f_\theta(x_i) \), with \( \theta \) being the parameters of the model. The loss function \( \mathcal{L}(\hat{y}_i, y_i) \) is defined to measure the performance for each example. The total loss over the dataset is:

\[
\mathcal{L}_{\text{total}} = \frac{1}{N} \sum_{i=1}^N \mathcal{L}(\hat{y}_i, y_i)
\]

The choice of loss function depends on the nature of the problem:

\begin{itemize}
    \item \textbf{Mean Squared Error (MSE):} Widely used in regression problems and defined as
    \begin{equation}
        \mathcal{L}_{\text{MSE}} = \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)^2
        \label{eq:mse_loss}
    \end{equation}
    This loss penalizes large deviations more heavily due to the square term, making it sensitive to outliers.

    \item \textbf{Binary Cross-Entropy (BCE):} Commonly used for binary classification tasks:
    \begin{equation}
        \mathcal{L}_{\text{BCE}} = -\frac{1}{N} \sum_{i=1}^N \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
        \label{eq:bce_loss}
    \end{equation}
    This loss function assumes \( \hat{y}_i \in (0, 1) \), making it suitable for models with a sigmoid output layer.

    \item \textbf{Categorical Cross-Entropy:} An extension of BCE used in multi-class classification problems with softmax output layers.
\end{itemize}

Loss functions not only guide the training process but also influence convergence properties and robustness. Therefore, in many cases, researchers build a custom loss function to handle the specific problem at hand which has even led to a separate field called the \textit{physics-informed machine learning}. 

\subsection{Backpropagation}

Upto this point, we have discussed many theoretical ideas involved in machine learning. However, one might question how does training process work, and how do we find the best parameters for the model. This is where the backpropagation and optimization algorithms come into play.

Let's first dig into the core idea of the backpropagation algorithm. 

\begin{tcolorbox}[colback=red!5!white, colframe=red!75!black]
    Backpropagation is a method for computing the gradient of the loss function with respect to the model parameters.
\end{tcolorbox}

As highlighted in equation \ref{eq:argmin_loss}, we want to obtain a set of parameters $\theta^*$ that minimizes the loss function $\mathcal{L}$ by updating the parameters like weights matrix $\textbf{W}$ and biases $\textbf{b}$. During this process, we iteratively update the parameters by computing the gradient of the loss function with respect to the parameters. The backpropagation algorithm offers a very computationally efficient approach to compute the partial derivatives of a complex, non-convex loss function in multilayer neural networks.  

This idea can be better understood when combined with optimization algorithms.

\subsection{Optimization Algorithms}

The whole basis of machine learning in build on the idea of paramter estimation for my neural network so that it can mimic the underlying function. This requires solving an optimization problem, where we try to find the values for a set of variables $\theta \in \Theta$, that minimize a scalar-valued loss function or cost function $\mathcal{L} : \Theta \rightarrow \mathbb{R}$:

\begin{equation}
    \theta^* = \operatorname*{argmin}_{\theta \in \Theta} \mathcal{L}(\theta)
    \label{eq:argmin_loss_opt}
\end{equation}

The algoithms involved in this optimization problem are called \textbf{solver}. There are multiple solvers available but the most common ones are:
\begin{itemize}
    \item \textbf{Gradient Descent:} A first-order optimization algorithm that iteratively updates the parameters in the direction of the steepest descent of the loss function.
    Upon initialization, the parameters are set to some random values $\theta_0$. Then at each iteration $t$, they perform an update of the following form:

    \begin{equation}
        \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}(\theta_t)
        \label{eq:gradient_descent}
    \end{equation}

    where, $\eta$ is the \textbf{learning rate}. The learning rate is another hyperparameter that controls the step size of the gradient descent. It is a crucial parameter that determines the speed and accuracy of the optimization process.

    Formally, we require that there exists an $\eta_{\text{max}} > 0$ such that:

    \begin{equation}
        \mathcal{L}(\theta_{t+1}) \leq \mathcal{L}(\theta_t)
        \label{eq:gradient_descent_bound}
    \end{equation}

    for all $\eta \in [0, \eta_{\text{max}}]$.


    \item \textbf{Stochastic Gradient Descent (SGD):} A variant of gradient descent that uses a small subset of the training data (mini-batch) to compute the gradient.
    We can write the update rule for SGD as:

    \begin{equation}
        \theta_{t+1} = \theta_t - \eta \nabla \mathcal{L}_i (\theta_t)
        \label{eq:sgd}
    \end{equation}

    where, $\mathcal{L}_i (\theta_t)$ is the loss function evaluated on the $i$-th training example.

    One should also note that stochastic gradient is an unbiased estimator of the true gradient:

    \begin{equation}
        \mathbb{E}[\nabla_{\theta} \mathcal{L}_i (\theta_t)] = \nabla_{\theta} \mathcal{L}(\theta_t)
        \label{eq:sgd_unbiased}
    \end{equation}

    Finally, we can use it on a mini-batch of size $\mathcal{B}$ to compute the gradient:

    \begin{equation}
        \textbf{g}_t \approx \frac{1}{|\mathcal{B}_t|} \sum_{i \in \mathcal{B}_t} \nabla_{\theta} \mathcal{L}_i(\theta_t)
        \label{eq:sgd_batch}
    \end{equation}

    and the update rule becomes:

    \begin{equation}
        \theta_{t+1} = \theta_t - \eta \textbf{g}_t
        \label{eq:sgd_batch_update}
    \end{equation}

    where, $\mathcal{B}_t$ is the mini-batch of size $\mathcal{B}$ at iteration $t$.

    \item \textbf{ADAM:} A second-order optimization algorithm that uses the gradient and the Hessian matrix to update the parameters.
    
    ADAM is a combination of lot of ideas from the field of optimization. To trace it back, we move:

    \begin{equation*}
        \textbf{AdaGrad} \rightarrow \textbf{RMSProp} \rightarrow \textbf{ADAM}
    \end{equation*}    

    I would recommend reading \cite{pml1Book} for more details on this approach. However, to break it down formally we introduce two new terms namely momentum and RMSProp:

    \begin{align}
        \textbf{m}_{t} &= \beta_1 \textbf{m}_{t-1} + (1 - \beta_1)\textbf{g}_t \\ 
        \textbf{s}_{t} &= \beta_2 \textbf{s}_{t-1} + (1 - \beta_2)\textbf{g}_t^2.
        \label{eq:adam_m_s}
    \end{align}

    The update rule translates to:
    
    \begin{equation}
        \theta_{t+1, d} = \theta_{t,d} - \eta_t \frac{1}{\sqrt{s_{t,d}} + \epsilon} \textbf{m}_{t, d}
        \label{eq:adam_update}
    \end{equation}

    where, $\beta_1$ and $\beta_2$ are the momentum and RMSProp parameters respectively.

\end{itemize}

Now, we can clearly find the application of the backpropagation algorithm in the context of neural networks. For dense neural networks, the gradient calculation could have been a nightmare. However, with the help of the backpropagation algorithm, we can compute the gradient of the loss function with respect to the parameters of the any neuron in the network.


\section{Gaussian Process Regression}






    














