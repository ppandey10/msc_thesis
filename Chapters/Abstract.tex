Analyzing gravitational wave (GW) data from compact binary mergers through Bayesian methods requires a deep dive into the posterior probability distribution of detected signals. Central to this process are accurate waveform models, which simulate the GW signals generated by systems defined by a specific set of parameters. Typically, sampling the posterior distribution for a single GW event demands the creation of around ten million waveforms, making the speed of waveform generation critically important. This urgency is amplified with the advent of next-generation GW detectors, which are poised to capture a far greater number of events than current instruments can. Optimizing the computational efficiency for these signals is vital for maximizing the scientific returns from large-scale experiments in the future.

One of the most promising strategies to achieve the required efficiency gains is template acceleration using machine learning. While there is an extensive body of literature on this approach for binary black holes, studies focusing on binary neutron stars remain comparatively scarce.

In my thesis research, I propose \texttt{mlgw\_bns\_HOM}, a neural network-based surrogate model designed to accelerate the generation of Effective-One-Body (EOB) waveforms while fully incorporating all higher-order modes (HOM) and precession effects for enhanced generalization and accuracy. This model achieves substantial computational gains through an innovative training algorithm that combines data compression techniques with analytical insights.